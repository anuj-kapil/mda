---
title: "Multivariate Data Analysis - Assignment 2"
output:
  word_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dpi=300)

# Install Packages
list.of.packages <- c("data.table"
                      ,"car"
                      ,"ggcorrplot"
                      ,"ggthemes"
                      ,"gridExtra"
                      ,"ggplot2"
                      ,"lars"
                      ,"MVN"
                      ,"MASS"
                      ,"mnormt"
                      ,"ICSNP"
                      ,"VGAM"
                      ,"nnet"
                      ,"rpart"
                      ,"glmnet"
                      ,"klaR"
                      ,"psych"
                      )
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies = T)

# Load Packages
library(data.table)
library(ggplot2)
library(gridExtra)
library(lars)
library(MVN)
library(ggthemes)
library(ggcorrplot)
library(car)
library(MASS)
library(mnormt)
library(ICSNP)
library(VGAM)
library(nnet)
library(rpart)
library(glmnet)
library(klaR)
library(psych)
```

## Multivariate Data Analysis Spring 2019 (37459-2019-SPRING-CITY)
### Assignment: 2
### Student Name: Anuj Kapil
### Student Id: 12678708

## Part A

## Question 1

For a given mean vector and covariance matrix, we can simulate random samples from the multivariate normal distribution in R using the 'mvrnorm' function from **MASS** package. 

```{r}
# Question 1
# Mean vector
mv<-rep(0, 3)

# Cov matrix
vcmat <- 1/5630 * matrix(c(575,-60,10,-60,300,-50,10,-50,196),nrow=3,byrow=TRUE)

# Covariance matrix
print(vcmat)

#MVN
mnd <- mvrnorm(n=1000,mv,vcmat)
```

### Question 1a

Calculate the least square estimates using R function for Y2 and Y3 where:
$$Y_2 = \beta_{2,1} Y_1 + \epsilon_2$$
and

$$Y_3 = \beta_{3,1} Y_1 + \beta_{3,2} Y_2 + \epsilon_3$$
Perform a linear regression to find the coefficients $\beta_{2,1}$, $\beta_{3,1}$ and $\beta_{3,2}$.

```{r}
# Question 1a

# Convert matrix to a data.table
mnd_df <- as.data.frame(as.table(mnd))
setDT(mnd_df)
mnd_dt <- dcast(mnd_df, Var1~Var2, value.var = 'Freq')
mnd_dt[,Var1:=NULL]

colnames(mnd_dt) <- c('Y1', 'Y2', 'Y3')

model_1<-lm(Y2~Y1, data = mnd_dt)

model_summary <- summary(model_1)

# Coefficent of Y1
beta2_1 <- model_summary$coefficients[[2]]
print(beta2_1)

model_2<-lm(Y3~Y1+Y2, data = mnd_dt)

model_summary <- summary(model_2)

#Coefficent of Y1
beta3_1 <- model_summary$coefficients[[2]]
print(beta3_1)

#Coefficent of Y2
beta3_2 <- model_summary$coefficients[[3]]
print(beta3_2)
```

### Question 1b

Estimate $\sigma_2^2 = var(\epsilon_2)$  

```{r}
# Question 1b
sigma_2_square <- (summary(model_1)$sigma)^2
print(sigma_2_square)
```

### Question 1c

Estimate $\sigma_3^2 = var(\epsilon_3)$  

```{r}
# Question 1c
sigma_3_square <- (summary(model_2)$sigma)^2
print(sigma_3_square)
```

### Question 1d

Construct the 3x3 matrix from coefficients

```{r}
T <- matrix(c(1,-1*beta2_1,-1*beta3_1,0,1,-1*beta3_2,0,0,1),nrow = 3)
print(T)
```

### Question 1e

Compute $T\sum T^\intercal$

```{r}
TT <- T%*%vcmat%*%t(T)

print(T)
```

### Question 1f
Calculate $S^{-1}$ given:  
$$S^{-1} = T^\intercal D^{-1} T$$
where $D$ is a 3x3 diagonal matrix, with entries on the main diagonal as ${\sigma_1^2}$, ${\sigma_2^2}$, ${\sigma_3^2}$ and $T$ has already been calculated earlier.

To make $S^{-1}$ as close to $\sum^{-1}$ possible, let us assume:

$${\sum}^{-1} = T^\intercal D^{-1} T$$
Based on that we can calculate the value of $D$ as: 

$$D =  {(T {\sum}^{-1} T^\intercal)}^{-1}$$

```{r}
# Calculate the inverse of covariance matrix
vcmat_inv <- solve(vcmat)
round(vcmat_inv)
# Sigma square can be derived from estimated D
sigma_1_square <- solve(T%*%vcmat_inv%*%t(T))[1,1]

# Calculate D
D = matrix(c(sigma_1_square, 0, 0, 0, sigma_2_square, 0, 0, 0, sigma_3_square),nrow = 3)
print(D)

# Compute S inverse
print(round(t(T)%*%solve(D)%*%T))

#Compare with inverse of covariance matrix
print(round(vcmat_inv))
```

## Question 2

### Load the dataset from local storage

Load the dataset using 'fread' from **data.table** package.
The stock data consists of weekly returns of five different stocks. The weekly returns of each stock are defined as (current week closing price - previous week closing price)/(previous week closing price) for 103 successive weeks.

```{r}
dat <- fread('Data/stockdata.csv')
summary(dat)
```

The observations in the data appear to be independently distributed but the rate of return across stocks are correlated. Generally, one would expect the stocks would move together in response to general economic conditions. The correlation below shows a relationship between JPMorgan, Citibank and WellsFargo (banking stocks) and also relationship between Royal Dutch Shell and ExxonMobil (oil stocks)

```{r}
ggcorrplot(cor(dat, use = "pairwise.complete.obs"), hc.order = FALSE, type = "lower",
           ggtheme = ggthemes::theme_gdocs,
           colors = c("#ff7f0e", "white", "#1f83b4"),
           lab = TRUE)+
           theme(panel.grid.major=element_blank())
```

### Question 2a

Perform factor analysis using principal component analysis method.
Looking at the importance of the components, the first two principal components explains 80% of the variance in the data. The proportion of the variance explained by component 3 is less than 0.2 ($1/p$) ($p$ being 5 in this case). A rule of thumb suggests retaining only those components whose variances individually are greater than $1/p$. So, we will retain only the first two principal components. 

```{r}
# Principal component analysis method
dat_pc<-princomp(dat)
summary(dat_pc, loadings = TRUE)
```

Based on the importance of the components, we have seen that first two components explains most of the variance, hence we are going to perform factor analysis using m=2 factors.

```{r}
fact_pc<-principal(dat, nfactors=2,rotate="none")
print(fact_pc)
```

### Question 2b

Looking at the factor loadings, all of the stocks load highly on the first factor while the second factor shows contrasting loading of banking stocks (negative low loadings) to the oil stocks (positive high loadings). It is clear that the factor $F_1$ represents general economic conditions and can be called as *Market Factor*. The second factor $F_2$ seems to differentiate stocks in different industries and can be called as *Industry Factor*.

So, looking at the factor variances and loadings, it can be summarized that the weekly rates of stocks are determined by general market conditions and activities in the respective industries and some of it is explained by other/residual factors.

The chart below shows the same story where all the variables load highly on the first factor and first 3 variable load negatively on second factor while the last 2 variables load positively on second factor.

```{r}
factor.plot(fact_pc)
```

Let's look at the factor anlysis with rotated loadings. Here, we are using the same principal components analysis method with **varimax** rotation. We will still use two factors and looking at the loadings, we now see a different pattern. The banking stocks load heavily on the the factor 1 $F_1$ while the oil stocks load heavily on the second factor $F_2$. Factor 1 represent those economic forces that causes the bank stocks to move together while the Factor 2 represents the economic forces that affect the oil stocks. The chart confirms the above findings.

```{r}
fact_pc_rot <-principal(dat, nfactors=2,rotate="varimax")
print(fact_pc_rot)
factor.plot(fact_pc_rot)
```

### Question 2c
Table 9.8 on page 510 of Johnson and Wichern shows the unrotated and rotated factors for the same dataset obtained using the maximum likelihood method. In our previous steps we have used the principal component method for obtaining the factors. Here, we will use another method (without rotation) to calculate the factors. The unrotated factors using the maximum likelihood method shows that the oil stocks load heavily on the the factor 1 ($F_1$) while the banking stock load heavily on the second factor $F_2$. Factor 1 represent those economic forces that causes the oil stocks to move together while the Factor 2 represents the economic forces that affect the banking stocks.

```{r}
# Maximum likelihood method
fact_ml <- factanal(x = dat,factors = 2, rotation = 'none')
print(fact_ml)
```

The rotated factors using the maximum likelihood method are similar to the rotated factors using the principal component method and shows that the banking stocks load heavily on the the factor 1 ($F_1$ )while the oil stock load heavily on the second factor $F_2$.

```{r}
# Maximum likelihood method with rotation
fact_ml_rot <- factanal(x = dat,factors = 2, rotation = 'varimax')
print(fact_ml_rot)
```

## Question 3

### Load the dataset from local storage  

Data Descriptions:
Four measurements of male Egyptian skulls from 5 different time periods. Thirty skulls are measured from each time period.  
Number of observations: 150  
Variable Names:  

* MB: Maximal Breadth of Skull
* BH: Basibregmatic Height of Skull
* BL: Basialveolar Length of Skull
* NH: Nasal Height of Skull
* Year: Approximate Year of Skull Formation (negative = B.C., positive = A.D.)  


```{r}
# Load the dataset
egyptskull <- fread('Data/egyptskull.csv')

# Descriptive statistics
summary(egyptskull)

# Convert the dependent variable to a factor
egyptskull[, Epoch:= as.factor(Epoch)]
```

### Question 3a

Logistic regression is a statistical model used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Logistic regression is the appropriate regression analysis to conduct when the dependent variable is binary.  Like all regression analyses, the logistic regression is a predictive analysis. In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.

### Question 3b

Classification trees are used to predict membership of cases or objects into classes of a categorical dependent variable from their measurements on one or more predictor variables. Classification tree analysis has traditionally been one of the main techniques used in data mining. 

It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels.

In computer science, Decision tree learning uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).  

### Question 3c

Scatter Plot of Maximal Breadth of Skull with the Nasal Height of Skull.  
The scatter plot shows no clear linear relationship and does not have any patterns across different Epochs.  

```{r}
ggplot(egyptskull, aes(x=MB, y=NH, group=Epoch))+
  geom_point(aes(color=Epoch))
```


### Question 3d

Split the dataset in to train and test datasets. For multionomial regression, we need to create 5 different response variables to denote the five levels of Epoch categories.

```{r}
egyptskull[, Epoch_1:=ifelse(Epoch == 4000, 1, 0)]
egyptskull[, Epoch_2:=ifelse(Epoch == 3300, 1, 0)]
egyptskull[, Epoch_3:=ifelse(Epoch == 1850, 1, 0)]
egyptskull[, Epoch_4:=ifelse(Epoch == 200, 1, 0)]
egyptskull[, Epoch_5:=ifelse(Epoch == 150, 1, 0)]

egyptskull_train <- egyptskull[,.SD[1:25], by = list(Epoch)]
egyptskull_test <- egyptskull[,.SD[26:30], by = list(Epoch)]

egyptskull_train[, .N, by = list(Epoch)]
egyptskull_test[, .N, by = list(Epoch)]
```

**Perform LDA**

Looking at the chart, we can see few misclassifications on the training set. The error rate is high being over 70% on training set.

```{r}
#######   LDA
model_lda <- lda(Epoch ~ MB+BH+BL+NH, data=egyptskull_train)
plot(model_lda)

egyptskull_test$lda_predict<-predict(model_lda,egyptskull_test[,2:5])$class

partimat(as.factor(Epoch) ~ MB+BH+BL+NH, data=egyptskull_train,method="lda")
```

**Perform QDA**

Looking at the chart, we can see fewer misclassifications on the training set than LDA. The error rate is high being over 65% on training set but better than LDA

```{r}
######## QDA
model_qda<-qda(Epoch ~ MB+BH+BL+NH, data=egyptskull_train)
model_qda

egyptskull_test$qda_predict<-predict(model_qda,egyptskull_test[,2:5])$class

partimat(as.factor(Epoch) ~ MB+BH+BL+NH, data=egyptskull_train,method="qda")
```

**Perform Multinomial Logistic**  

Looking at the variable importance, none of the variable explain the variance in Epoch except for BL (Basialveolar Length of Skull). Even that explains only one class.  

```{r}
######  Multinomial Logistic 
model_mnl<-vglm(formula = cbind(Epoch_1,Epoch_2,Epoch_3,Epoch_4,Epoch_5) ~ MB+BH+BL+NH, family = multinomial, data = egyptskull_train)
summary(model_mnl)

predictions<-predict(model_mnl,newdata=egyptskull_test[,2:5],type="response")
egyptskull_test$pred_mnl<-apply(predictions,1,function(i) which.max(i) )

egyptskull_test[, pred_mnl:= c(4000, 3300, 1850, 200, 150)[pred_mnl]]
```

**Perform CART**  

The algorithm of the decision tree models works by repeatedly partitioning the data into multiple sub-spaces, so that the outcomes in each final sub-space is as homogeneous as possible. This approach is technically called recursive partitioning. The produced result consists of a set of rules used for predicting the outcome variable, which can be either:  

* a continuous variable, for regression trees
* a categorical variable, for classification trees  

The decision rules generated by the CART (Classification & Regression Trees) predictive model are generally visualized as a binary tree. 

The model's lowest error rate is 0.68 after 7 spilts.

Even after tuning the 'cp' parameter that is pruning the tree at the lowest error rate, the model performance does not improve.  

```{r,dpi=300, fig.width=15, fig.height=15}
######## CART
model_ct <- rpart(Epoch ~ MB+BH+BL+NH, data = egyptskull_train, method="class")
plot(model_ct)
text(model_ct, use.n=TRUE, all=TRUE, cex=.7)

plotcp(model_ct)

egyptskull_test$pred_ct<-predict(model_ct,egyptskull_test,type="vector")
egyptskull_test[, pred_ct:= c(4000, 3300, 1850, 200, 150)[pred_ct]]

model_ct$cptable

model_ct_fit<- prune(model_ct, cp=model_ct$cptable[which.min(model_ct$cptable[,"xerror"]),"CP"])

summary(model_ct_fit)

plot(model_ct_fit)
text(model_ct_fit, use.n=TRUE, all=TRUE, cex=.7)

plotcp(model_ct_fit)
egyptskull_test$pred_ct_fit<-predict(model_ct_fit,egyptskull_test,type="vector")
egyptskull_test[, pred_ct_fit:= c(4000, 3300, 1850, 200, 150)[pred_ct_fit]]
```

**Build a neural network**  

Building the network with 5 hidden layers. Adjusting the hidden layers to a lower or a higher number does not lower the error rate on the test set.

```{r}
##### Nnet
model_nnet<-nnet(Epoch ~ MB+BH+BL+NH, data = egyptskull_train,size=5,decay=0.1)
summary(model_nnet)
egyptskull_test$pred_nnet<-predict(model_nnet,egyptskull_test,type="class")
```

### Question 3e

Multinomial Logistic Regression error rate on the test set is the least. Confusion matrix shows the least amount of misclassifications in Multinomial Logistic Regression. Hence, the best fit model is Multinomial Logistic Regression.  

```{r}
# LDA
#confusion matrix
table(egyptskull_test$Epoch,egyptskull_test$lda_predict)
# error rate
mean(egyptskull_test$lda_predict != egyptskull_test$Epoch)

# QDA
#confusion matrix
table(egyptskull_test$Epoch,egyptskull_test$qda_predict)
# error rate
mean(egyptskull_test$qda_predict != egyptskull_test$Epoch)

# Multinomial
#confusion matrix
print(table(egyptskull_test$Epoch,egyptskull_test$pred_mnl))
# error rate
mean(egyptskull_test$pred_mnl != egyptskull_test$Epoch)

# CART
#confusion matrix
table(egyptskull_test$Epoch,egyptskull_test$pred_ct_fit)
# error rate
mean(egyptskull_test$pred_ct_fit != egyptskull_test$Epoch)

# Nnet
#confusion matrix
table(egyptskull_test$Epoch,egyptskull_test$pred_nnet)
# error rate
mean(egyptskull_test$pred_nnet != egyptskull_test$Epoch)
```

### Question 3f

The lowest error rate on the test set is using the Multinomial Logistic Regression. The error rate is .6. So, we are going to use this model for new predictions.

```{r}
####### Predict
egyptskull_val <- data.table(rbind(c(128, 143, 103, 50) 
                                  , c(129, 126, 91, 50)
                                  , c(130, 127, 99, 45)
                                  , c(130, 131, 98, 53)
                                  , c(134, 124, 91, 55)
                                  , c(130, 130, 104, 49)
                                  , c(134, 139, 101, 49)
                                  , c(136, 133, 91, 49)
                                  ))

names(egyptskull_val) <- names(egyptskull)[1:4]

# Use multinomial

predictions<-predict(model_mnl,newdata=egyptskull_val,type="response")

egyptskull_val$pred_mnl<-apply(predictions,1,function(i) which.max(i) )

egyptskull_val[, pred_mnl:= c(4000, 3300, 1850, 200, 150)[pred_mnl]]
print(egyptskull_val)
```

## Part B

## Question 1

### Load the dataset from web

```{r}
#url <- 'https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data'
#diabetes_orig <- fread(url, sep = '\t')

#fwrite(diabetes_orig, 'Data/diabetes.csv')

# 
# data(diabetes)
# Xmatrix <- diabetes$x
# yVector <- diabetes$y
# 

diabetes_orig <- fread('Data/diabetes.csv')
dim(diabetes_orig)

```

### Question 1a

**Least Absolute Shrinkage and Selection Operator**  

Using k-fold cross validation to find the optimal value of lamda.  
lambda.min is the ideal choice for lambda for getting the best fit for LASSO.  
A rule of thumb is to use lamba.1se for the optimal fit for LASSO

```{r}
Xmatrix <- as.matrix(diabetes_orig[,1:10])
yVector <- diabetes_orig$Y

cvfit <- cv.glmnet(Xmatrix , yVector)

plot(cvfit)

cvfit$lambda.min
coef(cvfit, s = "lambda.min")

cvfit$lambda.1se
coef(cvfit, s = "lambda.1se")
```

Trying out different values of lamda.  
lambda = 1  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_1 <- glmnet(Xmatrix , yVector , lambda=1)
coef(LASSOfit_1)
betaHat_1 <- as.numeric(LASSOfit_1$beta)
betaHat_1
y <- yVector
y_hat_cv <- predict(LASSOfit_1, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_1$dev.ratio
```
lambda = 2  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_2 <- glmnet(Xmatrix , yVector , lambda=2)
coef(LASSOfit_2)
betaHat_2 <- as.numeric(LASSOfit_2$beta)
betaHat_2
y <- yVector
y_hat_cv <- predict(LASSOfit_2, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_2$dev.ratio
```
lambda = 4  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_4 <- glmnet(Xmatrix , yVector , lambda=2)
coef(LASSOfit_4)
betaHat_4 <- as.numeric(LASSOfit_4$beta)
betaHat_4
y <- yVector
y_hat_cv <- predict(LASSOfit_4, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_4$dev.ratio
```
lambda = 0.5  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_0.5 <- glmnet(Xmatrix , yVector , lambda=0.5)
coef(LASSOfit_0.5)
betaHat_0.5 <- as.numeric(LASSOfit_0.5$beta)
betaHat_0.5
y <- yVector
y_hat_cv <- predict(LASSOfit_0.5, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_0.5$dev.ratio
```
lambda = 0.05  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_0.05 <- glmnet(Xmatrix , yVector , lambda=0.05)
coef(LASSOfit_0.05)
betaHat_0.05 <- as.numeric(LASSOfit_0.05$beta)
betaHat_0.05
y <- yVector
y_hat_cv <- predict(LASSOfit_0.05, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_0.05$dev.ratio
```

Calculating the max r-square from the k-fold cross validation.  
lambda.min comes out to be the ideal choice which yeilds the maximum r-squared  

```{r}
rsq <- 1 - cvfit$cvm/var(yVector)

plot(cvfit$lambda,rsq)

lambda_rsq <- data.table(cbind(lambda=cvfit$lambda,rsq))

lambda_rsq[rsq==max(rsq)]
```

lambda = 0.0735996  
Calculating the beta coefficients and r-square  

```{r}
LASSOfit_0.07 <- glmnet(Xmatrix , yVector , lambda=0.0735996)
coef(LASSOfit_0.07)
betaHat_0.07 <- as.numeric(LASSOfit_0.07$beta)
betaHat_0.07
y <- yVector
y_hat_cv <- predict(LASSOfit_0.07, Xmatrix)
rsq_lasso_cv <- cor(y, y_hat_cv)^2
rsq_lasso_cv
LASSOfit_0.07$dev.ratio
```

### Question 1b

With the higher values of lambda, we are getting lesser variables that explain most of the variance in the disease progression (dependent variable). However, the r-squared gets lower with the lesser variables. The best r-square is achieved at lambda = 0.0735996 and includes almost all the variables that explain most of the variance. It should be noted that the key variables are still the same as found through manual experiments of trying linear regressions with different combinations of the variable. At lambda = 0.0735996, the most significant variable is S5, followed by SEX, BMI, S4 and BP. The remaining variables explains very little of the variance in the disease progression (dependent variable). 

### Question 1c

Variable significance from LASSO model is similar to the ones obtained from multiple linear regression. However, comparing the beta coefficients from LASSO model with the coefficients from multiple linear regression, the coefficients from LASSO model are slightly higher than obtained from multiple linear regression. The r-squared from LASSO model is slightly higher at 0.5174173 as compared to 0.5062 from multiple linear regression. In a way, LASSO model is better as it presents a similar fit and provides an automated way of finding beta co-efficents whereas multiple linear regression requires manual trial and error experiments.

```{r}
diabetes_orig[, SEX:=as.factor(SEX)]
model <- lm(Y~S2+S4+BMI+BP+S5+SEX, data = diabetes_orig)
summary(model)
```

## Question 2

### Load the dataset
Seeds dataset contains data about four varieties of wheat capturing the hedonic characteristics of each variety of wheat. The  data  of  wheat  seeds  is  gathered  from  UCI website  which  is  a  great  dataset  repository.  The numbers  of  samples  of  wheat seeds  are  210  from three wheat classes Kama, Rosa  and Canadian are collected for classification process. Seven geometrical or morphological features of seeds are considered on the  basis  of  which  seeds  are  classified  into  three classes of wheat. 

```{r}
seeds <- fread('Data/seeds_dataset.txt', sep = "\t")
col_names_seeds <- c("area", "perimeter", "compactness", "length_of_kernel", "width_of_kernel", "asymmetry_coefficient", "length_of_kernel_groove", "wheat_type")
names(seeds) <- col_names_seeds
dim(seeds)
```

### Question 2a

Estimate of covariance matrix using sample covariance method:

$$Q = \frac {1}{n-1} \sum_{i=1}^n (x_i-\overline x)(x_i-\overline x)^T $$

```{r}
seeds_vcmat <- cov(seeds[,1:7])
print(seeds_vcmat)
```

### Question 2b

Another method is maximum likelihood estimate:

$$Q = \frac {1}{n} \sum_{i=1}^n (x_i-\overline x)(x_i-\overline x)^T $$

Simple cases, where observations are complete, can be dealt with by using the sample covariance matrix. The sample covariance matrix (SCM) is an unbiased and efficient estimator of the covariance matrix. If the random variable has normal distribution, the sample covariance matrix has Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate.  

If the sample size $n$ is small and the number of considered variables $p$ is large, the above empirical estimators of covariance and correlation are very unstable. Specifically, it is possible to furnish estimators that improve considerably upon the maximum likelihood estimate in terms of mean squared error. As an alternative, many methods have been suggested to improve the estimation of the covariance matrix. All of these approaches rely on the concept of shrinkage. This is implicit in Bayesian methods and in penalized maximum likelihood methods and explicit in the Stein-type shrinkage approach.

### Question 2c

i.) A zero entry in the precision matrix (the inverse of the covariance matrix) means the corresponding variables are independent given all the other variables. Finding the covariance matrix that fits the data and has a conveniently large number of zero entries in it's inverse matrix is known as Covariance Selection. Zeros in the inverse covariance matrix are desirable both for computational and conceptual reasons: they indicate conditional independence between variables, making the model smaller. Conditional independence constraints describe the sparsity pattern of the inverse covariance matrix $\sum^{-1}$, zeros showing the conditional independence between variables. So, will choose the maximum likelihood estimate method. Maximizing the log-likelihood with respect to $\sum^{-1}$ leads to the maximum likelihood estimate $\hat{\sum}^{-1} = S^{-1}$ which isn’t usually sparse (here S is the sample covariance obtained from the data). Also when $p > n$, $S$ will be singular and so the maximum likelihood estimate cannot be computed.

ii.) This is the similar case which we had in Part A Q1 (f) where two of the off-diagnol entries in the inverse of the covariance matrix are zero and we calculated $S^{-1}$ to be as close to $\sum^{-1}$


## Question 3

**Research Question**  

To build a multiclass classifier for automatically classifying different varieties of seeds into 3 classes of wheat. Manual apporach uses various geometrical or morphological features of seeds to identify the variety of wheat. Mathematical algorithms can be utilized to perform a qualitative research. Investigation will require data collection and analysis, and the methodology for this will include experimenting with 4 different mathematical methods for building a classifier.

**Required Dataset**

Seeds dataset contains data about 3 varieties of wheat capturing the hedonic characteristics of each variety of wheat. The numbers  of  samples  of  wheat seeds  are  210  from three wheat classes Kama, Rosa  and Canadian are collected for classification process. Seven geometrical or morphological features of seeds are considered on the  basis  of  which  seeds  can be classified  into  three classes of wheat. 

**Data Acquisition**  
The dataset has been gathered from UCI Machine Learning Repository: 

http://archive.ics.uci.edu/ml/datasets/seeds


**Research Objective**  

To build a multiclass classifier for classifying the seven geometrical or morphological features of seeds into 3 classes of wheat.  

Machine Learning is widely used in the field of agriculture for differentiating the varieties of various crops and for identifying their quality as well. 

A  machine  vision  system  is  an  alternate  to  the manual inspection, in the field of biological sciences to analyze biological products. To classify the varieties of various food crops and for identifying their quality as well, the machine vision is broadly used in the field of agriculture. Machine algorithms can be used to identify different  varieties  of  wheat  seeds  to  classify  them according to their quality.  

In biology and agronomy crop seed characteristics are very significant aspects. Machine vision technology is  developed  to  quantify  the features,  the  quality precise examination and graduation of the crop seeds. A novel scheme is presented to  extract and quantify some of features having worth biologically.

**Methodology**  

The dataset has been gathered from UCI Machine Learning Repository. Data will be analysed for descriptive statistics, pre-processing and analyzing the univariate and multivariate distribution. The data will be checked to understand the linear relationship among various variables. Four different classification techniques will be used to fit a model with the best performance. LDA, QDA, Multinomial Logistic regression and CART Decision tress. The dataset will be split into training and test datasets which will be used to train the models and test the prediction performance of each of the models. The model performance will be compared to answer our research question of building an optimal classifier for classfying the 3 varieties of wheat.

**Descriptive Statistics**  

No missing observations identified in the dataset. The data has 210 observation with 7 features (independent variable) and 1 class of seeds variabe (dependent variable).

```{r}
summary(seeds)
dim(seeds)
```

**Multicollinearity Analysis**

The visualization shows that the independent variables are highly correlated with each other which may pose some complexities in building the models. 'asymmetry_coefficient' is the only variable that is correlated with the dependent variable 'wheat_type'. From the chart, it is concluded that there is high linear relationship among independent variables.

```{r,dpi=300, fig.width=7, fig.height=7}
#Corr Plot
corr <- cor(seeds, use = "pairwise.complete.obs")

ggcorrplot(corr, hc.order = FALSE, type = "lower",
           ggtheme = ggthemes::theme_gdocs,
           colors = c("#ff7f0e", "white", "#1f83b4"),
           lab = TRUE)+
           theme(panel.grid.major=element_blank())
```

**Univariate Analysis**  

Shapiro Wilk test rejects the null hypothesis all variables being univariate normal. Hence, none of the variables are normally distributed.

```{r}

options(scipen = 999)
# Normal Q-Q plot for area
qqnorm(seeds$area, sub = colnames(seeds)[1])
ggplot(seeds, aes(x=area)) + geom_density()

# Shapiro Wilk test for variable area
shapiro.test(seeds$area)

# Normal Q-Q plot for perimeter
qqnorm(seeds$perimeter, sub = colnames(seeds)[2])
ggplot(seeds, aes(x=perimeter)) + geom_density()

# Shapiro Wilk test for variable perimeter
shapiro.test(seeds$perimeter)

# Normal Q-Q plot for compactness
qqnorm(seeds$compactness, sub = colnames(seeds)[3])
ggplot(seeds, aes(x=compactness)) + geom_density()

# Shapiro Wilk test for variable compactness
shapiro.test(seeds$compactness)

# Normal Q-Q plot for length_of_kernel
qqnorm(seeds$length_of_kernel, sub = colnames(seeds)[4])
ggplot(seeds, aes(x=length_of_kernel)) + geom_density()

# Shapiro Wilk test for variable length_of_kernel
shapiro.test(seeds$length_of_kernel)

# Normal Q-Q plot for width_of_kernel
qqnorm(seeds$width_of_kernel, sub = colnames(seeds)[5])
ggplot(seeds, aes(x=width_of_kernel)) + geom_density()

# Shapiro Wilk test for variable width_of_kernel
shapiro.test(seeds$width_of_kernel)

# Normal Q-Q plot for asymmetry_coefficient
qqnorm(seeds$asymmetry_coefficient, sub = colnames(seeds)[6])
ggplot(seeds, aes(x=asymmetry_coefficient)) + geom_density()

# Shapiro Wilk test for variable asymmetry_coefficient
shapiro.test(seeds$asymmetry_coefficient)

# Normal Q-Q plot for length_of_kernel_groove
qqnorm(seeds$length_of_kernel_groove, sub = colnames(seeds)[7])
ggplot(seeds, aes(x=length_of_kernel_groove)) + geom_density()

# Shapiro Wilk test for variable length_of_kernel_groove
shapiro.test(seeds$length_of_kernel_groove)
```


**Multivariate Normality test**

The data is not multivariate normal as per the royston test.

```{r}
mvtest <- mvn(seeds[,1:7], mvnTest='royston', multivariatePlot='qq')
mvtest$multivariateNormality
mvtest$univariateNormality
mvtest$Descriptives

```

### Transform to near normal

```{r, eval = TRUE}
trans<-powerTransform(seeds[,1:7])
seeds_trans <- seeds[,1:7]
seeds_trans<-bcPower(seeds_trans,trans$lambda)
mvtest_trans <- mvn(seeds_trans, mvnTest='royston', multivariatePlot='qq')
mvtest_trans$multivariateNormality
mvtest_trans$univariateNormality
mvtest_trans$Descriptives
```

**Data Preprocessing**  

Split the dataset in to train and test datasets. For multionomial regression, we need to create 3 different response variables to denote the three levels of wheat categories.  

```{r}
# Convert the dependent variable to a factor
seeds[, wheat_type:= as.factor(wheat_type)]

seeds[, wheat_type_1:=ifelse(wheat_type == 1, 1, 0)]
seeds[, wheat_type_2:=ifelse(wheat_type == 2, 1, 0)]
seeds[, wheat_type_3:=ifelse(wheat_type == 3, 1, 0)]

seeds_train <- seeds[,.SD[1:55], by = list(wheat_type)]
seeds_test <- seeds[,.SD[56:70], by = list(wheat_type)]

seeds_train[, .N, by = list(wheat_type)]
seeds_test[, .N, by = list(wheat_type)]
```

**LDA**  
Performing a Linear discriminant analysis on the dataset.
Looking at the chart, we can see very few misclassifications on the test set. The error rate is quite low being 18% on the test set, which is very good.

```{r,dpi=300, fig.width=15, fig.height=15}
#######   LDA
model_lda <- lda(wheat_type ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, data=seeds_train)
plot(model_lda)

seeds_test$lda_predict<-predict(model_lda,seeds_test[,2:8])$class

partimat(as.factor(wheat_type) ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, data=seeds_train,method="lda")

# LDA
#confusion matrix
table(seeds_test$wheat_type,seeds_test$lda_predict)
# error rate
mean(seeds_test$lda_predict != seeds_test$wheat_type)
```

**QDA**  

Performing a Quadratic discriminant analysis on the dataset.
Looking at the chart, we can see more misclassifications on the test set than LDA. The error rate is higher being 27% on the test set. Linear discrimination works better on this dataset than quadratic in separating the 3 classes of wheat.  

```{r,dpi=300, fig.width=15, fig.height=15}
######## QDA
model_qda<-qda(wheat_type ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, data=seeds_train)
model_qda

seeds_test$qda_predict<-predict(model_qda,seeds_test[,2:8])$class

partimat(wheat_type ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, data=seeds_train, method="qda")

# QDA
#confusion matrix
table(seeds_test$wheat_type,seeds_test$qda_predict)
# error rate
mean(seeds_test$qda_predict != seeds_test$wheat_type)
```

**Multinomial logistic**  

Residual deviance of the model is low which indicates a good fit. The error rate is 0.13 on the test dataset, which is the best so far.

```{r warning=FALSE, message=FALSE}
model_mnl<-vglm(formula = cbind(wheat_type_1,wheat_type_2,wheat_type_3) ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, family = multinomial, data = seeds_train)
summary(model_mnl)

predictions<-predict(model_mnl,newdata=seeds_test[,2:8],type="response")
seeds_test$pred_mnl<-apply(predictions,1,function(i) which.max(i) )

#confusion matrix
print(table(seeds_test$wheat_type,seeds_test$pred_mnl))

# error rate
mean(seeds_test$pred_mnl != seeds_test$wheat_type)
```

**CART**  

The model's lowest error rate is 0.06363636 after 2 spilts but the error rate on the test set is 0.22. The model seems to be overfitting.

```{r,dpi=300, fig.width=15, fig.height=15}
######## CART
model_ct <- rpart(wheat_type ~ area+perimeter+compactness+length_of_kernel+width_of_kernel+asymmetry_coefficient+length_of_kernel_groove, data = seeds_train, method="class")
plot(model_ct)
text(model_ct, use.n=TRUE, all=TRUE, cex=.7)

plotcp(model_ct)

seeds_test$pred_ct<-predict(model_ct,seeds_test,type="vector")
#confusion matrix
table(seeds_test$wheat_type,seeds_test$pred_ct)

# error rate
mean(seeds_test$pred_ct != seeds_test$wheat_type)
```


**Conclusion**

Based on the model's perfomance of all four models, the Multinomial logistic regression model is the best fitting model and is the recommend classifier for the research question. The seven geometrical or morphological features are apt for classifying the variety of wheat. It is recommend that at least these seven geometrical or morphological features be collected for classifying the variety of seeds. Further research can be done by analyzing more environmental factor that can impact the geometrical or morphological features of the seeds. For example, taking the seeds samples from different feilds and harvested using different methods. Capturing the enivromental and harvesting methods data and analysing it for classification may yeild in better classification models and generalizing the techniques over the population.